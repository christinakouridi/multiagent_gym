import datetime as dt
from pathlib import Path
import time
from typing import List

import numpy as np
from torch.utils.tensorboard import SummaryWriter

import gym
import ma_gym

from agent import DQNAgent
from agent_args import AGENT_ARGS

import warnings
warnings.filterwarnings("ignore")


def train_ma(
        agent_training_episodes: int,
        joint_training_episodes: int,
        env_name: str,
        agent_kwargs: dict,
        log_every: int = 500,
        render: bool = True
):

    env = gym.make(env_name)

    agent_kwargs['observation_space_dim'] = env.observation_space[0].shape[0]
    agent_kwargs['n_actions'] = env.action_space[0].n

    agents = [DQNAgent(**agent_kwargs) for _ in range(env.n_agents)]

    print('Agents initialised. Training...')

    agent_has_trained = [False for _ in agents]

    # Construct the total episode count as a combination of agent-specific training
    # And a sequence of joint training steps at the end
    n_episodes = agent_training_episodes * env.n_agents + joint_training_episodes
    training_chunk_borders = np.cumsum([agent_training_episodes] * env.n_agents + [joint_training_episodes])
    reward_chunk_borders = [-20. + 5.*(a+1) for a in range(env.n_agents)] + [100.]
    current_chunk = 0
    print(reward_chunk_borders)

    episode_rewards = []
    start_time = time.time()

    for episode in range(1, n_episodes + 1):

        # Advance chunks when we pass the border definition
        # Generated by the cumsum over our episodes per chunk
        if episode > training_chunk_borders[current_chunk]:
            # Up until the final training chunk we're training a specific agent
            if current_chunk <= len(agents):
                agent_has_trained[current_chunk] = True
                print('Finished training agent', current_chunk)

            current_chunk += 1
            print('Now training chunk', current_chunk)

        obs_n = env.reset()
        done_n = [False for _ in agents]
        agent_losses = 0
        ep_reward = 0
        ep_step = 0

        while not all(done_n):
            action_n = [-1 for _ in agents]
            for a, agent in enumerate(agents):
                # Unless the agent has already trained, or we're currently training the agent, hold to a no-op
                if not agent_has_trained[a] and current_chunk != a:
                    action_n[a] = 4  # noop
                elif agent_has_trained[a]:
                    action_n[a] = agent.act(np.array(obs_n[a]), evaluate=True)
                else:
                    action_n[a] = agent.act(np.array(obs_n[a]), evaluate=False)

            next_obs_n, reward_n, done_n, info = env.step(action_n)
            if render and not episode % log_every:
                env.render()
                time.sleep(0.1)

            ep_reward += sum(reward_n)

            for a, agent in enumerate(agents):
                if current_chunk == a:
                    loss = agent.step(
                        state=np.array(obs_n[a]),
                        action=action_n[a],
                        reward=reward_n[a] if not done_n[a] else PER_AGENT_REWARD,
                        next_state=np.array(next_obs_n[a]),
                        done=done_n[a]
                    )

                    agent_losses += loss if loss else 0

            obs_n = next_obs_n
            ep_step += 1

        episode_rewards.append(ep_reward)

        TB_WRITER.add_scalar('Loss', agent_losses, episode)
        TB_WRITER.add_scalar('Episode reward', ep_reward, episode)
        TB_WRITER.add_scalar('Epsilon', agents[0].epsilon, episode)

        if not episode % log_every:
            current_time = time.time()

            if render:
                time.sleep(0.2)  # pause to see final state

            print(f'Ep: {episode} / '
                  f'(Last {log_every:,.0f}) Mean: {np.mean(episode_rewards[-log_every:]):.1f} / '
                  f'Min: {np.min(episode_rewards[-log_every:]):.1f} / '
                  f'Max: {np.max(episode_rewards[-log_every:]):.1f} / '
                  f'EPS: {episode / (current_time - start_time):.1f} / '
                  f'Agent epsilon: {agents[0].epsilon:.2f}'
                  )

    print('Done training!\n')
    env.close()

    return agents, episode_rewards


def test_ma(agents: List[DQNAgent], test_eps):
    env = gym.make(ENV_NAME)
    ep_rewards = []

    for test_ep in range(test_eps):
        obs_n = env.reset()
        done_n = [False for _ in agents]

        ep_reward = 0
        ep_step = 0

        while not all(done_n):

            action_n = [agent.act(np.array(obs_n[a]), evaluate=True)
                        for a, agent in enumerate(agents)]  # note hacked obs space
            next_obs_n, reward_n, done_n, _ = env.step(action_n)
            env.render()

            obs_n = next_obs_n

            ep_reward += sum(reward_n)
            ep_step += 1

        ep_rewards.append(ep_reward)
        time.sleep(0.5)

    print('\n')
    print('=== Test performance ===')
    print(f'Mean: {np.mean(ep_rewards):.1f} / '
          f'Min: {np.min(ep_rewards):.1f} / '
          f'Max: {np.max(ep_rewards):.1f}')

    env.close()
    return ep_rewards


if __name__ == '__main__':

    ENV_NAME = 'Switch2-v0'
    MODEL_NAME = 'AC-LINx3-128'

    LOG_EVERY = 50
    STEPS_PER_EPISODE = 50
    PER_AGENT_REWARD = 5.0

    LOGGING_DEST = Path.cwd().joinpath(
        Path(f"logs/{MODEL_NAME}-{ENV_NAME}-{dt.datetime.now().strftime('%y%m%d-%H%M%S')}"))

    TB_WRITER = SummaryWriter(str(LOGGING_DEST))

    AGENT_TRAINING_EPISODES = 500
    JOINT_TRAINING_EPISODES = 500

    print('Beginning training')
    print('Logging to:', LOGGING_DEST)

    trained_agents, training_rewards = train_ma(AGENT_TRAINING_EPISODES, JOINT_TRAINING_EPISODES, ENV_NAME, AGENT_ARGS,
                                                log_every=LOG_EVERY, render=True)
    test_ma(trained_agents, 5)
